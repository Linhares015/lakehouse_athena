# lakehouse_athena

Este repositÃ³rio provÃª um exemplo simples de ambiente **Lakehouse** rodando localmente via Docker Compose. Ele inclui os principais serviÃ§os necessÃ¡rios para experimentar um fluxo analÃ­tico completo:

- **MinIO** atuando como storage compatÃ­vel com S3
- **PostgreSQL** usado pelo **Hive Metastore**
- **Hive Metastore** fornecido pela imagem `apache/hive:3.1.3`
- **Spark** (master e worker) com suporte ao **Delta Lake**
- **Trino** para consulta aos dados
- **Prometheus**, **Grafana** e **Node Exporter** para observaÃ§Ã£o

## Requisitos

- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/)

## Estrutura de diretÃ³rios

- `minio/data` â€“ armazenamento persistente do MinIO
- `postgres/data` â€“ dados do PostgreSQL utilizado pelo Hive Metastore
- `spark/conf` â€“ arquivos de configuraÃ§Ã£o do Spark (ex.: `spark-defaults.conf`)
- `trino/catalog` â€“ catÃ¡logos utilizados pelo Trino
- `monitoring/prometheus.yml` â€“ configuraÃ§Ã£o do Prometheus
- `scripts` â€“ utilitÃ¡rios para iniciar ou parar o ambiente

## InicializaÃ§Ã£o rÃ¡pida

Clone este repositÃ³rio e execute:

```bash
./scripts/start.sh
```

Os contÃªineres serÃ£o criados em segundo plano. Para encerrÃ¡-los use `./scripts/stop.sh`.

## Acessando os serviÃ§os

- MinIO: <http://localhost:9001> (usuÃ¡rio **minio** / senha **minio123**)
- Spark Master: <http://localhost:8080>
- Trino: <http://localhost:8088>
- Prometheus: <http://localhost:9090>
- Grafana: <http://localhost:3000>

## Exemplos de uso

Listar catÃ¡logos disponÃ­veis no Trino:

```bash
docker-compose exec trino trino --execute 'SHOW CATALOGS;'
```

Executar um comando simples no Spark:

```bash
docker-compose exec spark-master spark-sql -e "SHOW DATABASES;"
```

## Diagrama simplificado da infraestrutura

```mermaid
flowchart LR
    subgraph Monitoring
        NodeExporter --> Prometheus
        Prometheus --> Grafana
    end
    SparkMaster --> HiveMetastore
    SparkWorker --> SparkMaster
    Trino --> HiveMetastore
    HiveMetastore --> Postgres
    MinIO --> HiveMetastore
    MinIO --> SparkMaster
    MinIO --> SparkWorker
    MinIO --> Trino
```

Com esse ambiente Ã© possÃ­vel testar pipelines com Spark/Delta Lake e realizar consultas usando Trino, alÃ©m de monitorar a saÃºde de cada serviÃ§o via Grafana.



rodar notebooks pyspark no vscode:

ğŸ’¡ Por que vocÃª estÃ¡ travando nisso?
âœ… O VSCode Notebooks (Jupyter) precisa de um kernel com ipykernel instalado.
âœ… O kernel precisa saber onde estÃ¡ o Spark real (seja via PySpark, seja via findspark).
âœ… VocÃª criou venv para rodar o kernel com Jupyter, mas o Spark REAL nÃ£o vem do pip install pyspark.
âœ… Quando tenta usar o Python global, o VSCode reclama porque nÃ£o tem ipykernel no global e vocÃª nÃ£o consegue instalar (externally-managed environment, PEP 668).

âš  Resumo: o VSCode/Jupyter quer ipykernel â†’ vocÃª sÃ³ consegue em venv â†’ mas seu Spark estÃ¡ fora do venv.

ğŸš€ SoluÃ§Ã£o limpa e que vai funcionar no seu cenÃ¡rio
â¡ Mantenha o Spark fora do venv, como vocÃª baixou (binÃ¡rio)
â¡ Use um venv para o Jupyter/ipykernel e findspark (nÃ£o instale pyspark no venv)
â¡ Configure o findspark apontando para o Spark binÃ¡rio na primeira cÃ©lula do notebook

ğŸ“ Passo a passo final
1ï¸âƒ£ Crie o venv e instale o Jupyter + findspark:

bash
Copiar
Editar
python3 -m venv venv_spark
source venv_spark/bin/activate
pip install ipykernel jupyterlab findspark
(âœ¨ nÃ£o instale pyspark no venv!)

2ï¸âƒ£ Adicione o kernel:

bash
Copiar
Editar
python -m ipykernel install --user --name=venv_spark --display-name "Spark (venv)"
3ï¸âƒ£ No seu notebook, a primeira cÃ©lula:

python
Copiar
Editar
import findspark
findspark.init("/home/linhares/lakehouse_athena/spark")

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Test").getOrCreate()

print("Spark Version:", spark.version)
spark.range(5).show()